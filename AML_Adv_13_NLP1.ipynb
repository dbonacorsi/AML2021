{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AML_Adv_13_NLP1.ipynb","provenance":[{"file_id":"1vw7tXcpq01hJlvRRvqPb-ZqEJ7Zzx85m","timestamp":1579365091708}],"collapsed_sections":["A9rn6KrQgmCa","GydXeKSHgi4t","TGnRRVRCrVnO","LmzU1aZG0sJS"],"authorship_tag":"ABX9TyPbxVv23tYkA4tlJA7WBj15"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EBnicMZDxY_P"},"source":["# <font color='red'>Text processing - Part 1</font>"]},{"cell_type":"markdown","metadata":{"id":"A9rn6KrQgmCa"},"source":["## Load data"]},{"cell_type":"code","metadata":{"id":"1ie_6K4ofomJ"},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tDRSSDVygB8f"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h3rezWlkg03w"},"source":["!head -2 metamorphosis.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9oqyF452gs4k"},"source":["# load text\n","filename = 'metamorphosis.txt'\n","file = open(filename, 'rt')\n","text = file.read()\n","file.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NB2KdDEIgzXA"},"source":["text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GydXeKSHgi4t"},"source":["##Tokenization: manually"]},{"cell_type":"code","metadata":{"id":"PVWeemXWg_9q"},"source":["words_1 = text.split()\n","print(words_1[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKD3FAMRit2e"},"source":["import re\n","words_2 = re.split(r'\\W+', text)\n","print(words_2[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kIBv0rMuj9gM"},"source":["# import string library function \n","import string \n","print(string.punctuation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YbAGbsbRlwf8"},"source":["re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n","# remove punctuation from each word\n","naked_words = [re_punc.sub('', w) for w in words_1]\n","print(naked_words[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oFrel8mGosuj"},"source":["# convert to lower case\n","naked_lower_words = [word.lower() for word in naked_words]\n","print(naked_lower_words[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TGnRRVRCrVnO"},"source":["## Tokenization + cleaning: with NLTK"]},{"cell_type":"code","metadata":{"id":"kAqFlMsfrYaR"},"source":["import nltk\n","nltk.download()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GG5MVt1-s_ds"},"source":["from nltk import sent_tokenize\n","# split into sentences\n","sentences = sent_tokenize(text)\n","#\n","sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gG2VoZQut9sJ"},"source":["print(sentences[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rpqivIb1t1vY"},"source":["print(sentences[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qfHuh6Cst55Z"},"source":["from nltk.tokenize import word_tokenize\n","# split into words\n","tokens = word_tokenize(text) \n","print(tokens[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"50eNZ9HuvKBL"},"source":["# remove all tokens that are not alphabetic\n","words = [word for word in tokens if word.isalpha()] \n","print(words[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4KcONQPbwgYR"},"source":["from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","print(stop_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cX-i1Q3myxtL"},"source":["It is time to see a possible pipeline:\n","\n","(NOTE: do not execute all cells without looking at what they do.. this is a duplication of things done above.. you could mess up by re executing cells without checking firts..)"]},{"cell_type":"code","metadata":{"id":"wwD0ir2myzd5"},"source":["## possible pipeline  - careful if you want to rerun it, you actually did everytthing already above!\n","\n","import string\n","import re\n","\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","# load data\n","### see above\n","\n","# split into words\n","tokens = word_tokenize(text)\n","\n","# convert to lower case\n","tokens = [w.lower() for w in tokens]\n","\n","# prepare regex for char filtering\n","# remove punctuation from each word\n","re_punc = re.compile('[%s]' % re.escape(string.punctuation)) \n","stripped = [re_punc.sub('', w) for w in tokens]\n","\n","# remove remaining tokens that are not alphabetic\n","words = [word for word in stripped if word.isalpha()]\n","\n","# filter out stop words\n","stop_words = set(stopwords.words('english'))\n","words = [w for w in words if not w in stop_words]\n","\n","print(words[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LmzU1aZG0sJS"},"source":["## Stemming of words"]},{"cell_type":"code","metadata":{"id":"EfuHQ0vCzFO5"},"source":["from nltk.tokenize import word_tokenize\n","from nltk.stem.porter import PorterStemmer\n","\n","# load data\n","### see above\n","\n","# split into words\n","tokens = word_tokenize(text)\n","\n","# stemming of words\n","porter = PorterStemmer()\n","stemmed = [porter.stem(word) for word in tokens] \n","\n","print(stemmed[:100])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l5jiaP-A1Ext"},"source":["Note that we got commas back, because we are out of the pipeline, for code simplicity. You can insert stemming into the pipeline, of course."]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.16"},"colab":{"name":"AML_Adv_2_SVM.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Xfi_boTqVWl8"},"source":["# **Support Vector Machines** (SVM)"]},{"cell_type":"markdown","metadata":{"id":"qrfEJ6VwWuDw"},"source":["Import common modules. Make sure matplotlib plots figures inline. Check Python 3 or later is installed (Python 2.x may work, but it is deprecated in colab, so better to move to v3). Check sklearn ≥0.20 is installed."]},{"cell_type":"code","metadata":{"id":"tZuQjwLKXBUz"},"source":["# Python ≥3 is required\n","import sys\n","assert sys.version_info >= (3)\n","#for py3.5: assert sys.version_info >= (3, 5)\n","\n","\n","# Scikit-Learn ≥0.20 is required\n","import sklearn\n","assert sklearn.__version__ >= \"0.20\"\n","\n","# Common imports\n","import numpy as np\n","import os\n","\n","# to make the notebook's output stable across subsequent runs\n","np.random.seed(42)\n","\n","# To plot pretty figures\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","mpl.rc('axes', labelsize=14)\n","mpl.rc('xtick', labelsize=12)\n","mpl.rc('ytick', labelsize=12)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hTghYHwzdd0C"},"source":["# Linear SVM"]},{"cell_type":"code","metadata":{"id":"JwtcBgChc-uo"},"source":["import numpy as np\n","from sklearn import datasets\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import LinearSVC\n","\n","iris = datasets.load_iris()\n","X = iris[\"data\"][:, (2, 3)]  # petal length, petal width\n","y = (iris[\"target\"] == 2).astype(np.float64)  # Iris virginica\n","\n","svm_clf = Pipeline([\n","        (\"scaler\", StandardScaler()),\n","        (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", random_state=42)),\n","    ])\n","\n","svm_clf.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_3zQ0Y6dLzq"},"source":["svm_clf.predict([[5.5, 1.7]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q7FP0QHfdhr-"},"source":["# Non-linear SVM"]},{"cell_type":"code","metadata":{"id":"31BchT3adMH3"},"source":["from sklearn.datasets import make_moons\n","X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n","\n","def plot_dataset(X, y, axes):\n","    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n","    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n","    plt.axis(axes)\n","    plt.grid(True, which='both')\n","    plt.xlabel(r\"$x_1$\", fontsize=20)\n","    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n","\n","plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wiq4xDCkdm78"},"source":["from sklearn.datasets import make_moons\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures\n","\n","polynomial_svm_clf = Pipeline([\n","        (\"poly_features\", PolynomialFeatures(degree=3)),\n","        (\"scaler\", StandardScaler()),\n","        (\"svm_clf\", LinearSVC(C=100, loss=\"hinge\", random_state=42))\n","    ])\n","\n","polynomial_svm_clf.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYshSp-Vd0Gp"},"source":["def plot_predictions(clf, axes):\n","    x0s = np.linspace(axes[0], axes[1], 100)\n","    x1s = np.linspace(axes[2], axes[3], 100)\n","    x0, x1 = np.meshgrid(x0s, x1s)\n","    X = np.c_[x0.ravel(), x1.ravel()]\n","    y_pred = clf.predict(X).reshape(x0.shape)\n","    y_decision = clf.decision_function(X).reshape(x0.shape)\n","    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n","    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n","\n","plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n","plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2x-ismfKd3SH"},"source":["from sklearn.svm import SVC\n","\n","poly_kernel_svm_clf = Pipeline([\n","        (\"scaler\", StandardScaler()),\n","        (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n","    ])\n","poly_kernel_svm_clf.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L8YF9XgYeBam"},"source":["poly100_kernel_svm_clf = Pipeline([\n","        (\"scaler\", StandardScaler()),\n","        (\"svm_clf\", SVC(kernel=\"poly\", degree=10, coef0=100, C=5))\n","    ])\n","poly100_kernel_svm_clf.fit(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"agwUcOU9eI6l"},"source":["fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)\n","\n","plt.sca(axes[0])\n","plot_predictions(poly_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\n","plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\n","plt.title(r\"$d=3, r=1, C=5$\", fontsize=18)\n","\n","plt.sca(axes[1])\n","plot_predictions(poly100_kernel_svm_clf, [-1.5, 2.45, -1, 1.5])\n","plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\n","plt.title(r\"$d=10, r=100, C=5$\", fontsize=18)\n","plt.ylabel(\"\")\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KzqIPnFd769n"},"source":["# <font color=red>Exercise 1</font> (easy)"]},{"cell_type":"markdown","metadata":{"id":"xRLBGt9L79Ds"},"source":["Find a way to convince yourself of the different performances of SVC with polynomial kernel trick, at various parameter values. Make tests to gain some confidence as of which is the impact on timing coming from # instances, or from the choice of the polynomial degree, or from the choice of the parameter that drive the influence of higher order polynomials on the model, or..\n","\n","*   _HINT_: find simplest thing(s) to change, one at a time, and use the `%%time` magic function in the revelant cells.."]},{"cell_type":"markdown","metadata":{"id":"tfQcsE_5fv28"},"source":["# <font color=red>Exercise 2</font> (intermediate)"]},{"cell_type":"markdown","metadata":{"id":"_AnVGqbjfvo0"},"source":["Train a `LinearSVC` on a linearly separable dataset (e.g. take the IRIS dataset). Then train an `SVC` and a `SGDClassifier` on the same dataset. Try to verify that you are able to get them to produce \"roughly\" the same model (not precisely the same of course, but effectively same separating power..)"]},{"cell_type":"markdown","metadata":{"id":"KCVhePEbfvlj"},"source":["## <font color='green'>Solution</font>"]},{"cell_type":"code","metadata":{"id":"rBwz7qPVg-de"},"source":["# type your code below"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DyXnXpclezsG"},"source":["_Credits: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (2nd Edition) by Aurélien Géron, O'Reilly Media Inc., 2019_"]}]}
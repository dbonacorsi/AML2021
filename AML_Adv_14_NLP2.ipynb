{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AML_Adv_14_NLP2.ipynb","provenance":[{"file_id":"1vw7tXcpq01hJlvRRvqPb-ZqEJ7Zzx85m","timestamp":1579365091708}],"collapsed_sections":["2gP44aUQxIzS","p-g9GvyABfUT","1ETwtMhvBmQy","ioPsvLdBHcnS","vgYXf6NzxOcp","BKhRHrLaSgdh"],"authorship_tag":"ABX9TyPW9r0h44hgs92q7plTFpiX"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"boYXDzZbu5ag"},"source":["# <font color='red'>Text processing - Part 2</font>"]},{"cell_type":"markdown","metadata":{"id":"2gP44aUQxIzS"},"source":["# Handle text with sklearn"]},{"cell_type":"markdown","metadata":{"id":"p-g9GvyABfUT"},"source":["## CountVectorizer"]},{"cell_type":"code","metadata":{"id":"BLVb-W2X09cz"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# list of text documents\n","text = [\"The quick brown fox jumped over the lazy dog.\"]\n","\n","# create the transform\n","vectorizer = CountVectorizer()\n","\n","# tokenize and build vocab\n","vectorizer.fit(text)\n","\n","# summarize\n","print(vectorizer.vocabulary_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vj5BrY079BDF"},"source":["# encode document\n","vector = vectorizer.transform(text)\n","\n","# summarize encoded vector\n","print(vector.shape)\n","print(type(vector))\n","print(vector.toarray())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LMrvFFlv9SrB"},"source":["# encode another document\n","text2 = [\"the puppy\"]\n","vector = vectorizer.transform(text2)\n","print(vector.toarray())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ETwtMhvBmQy"},"source":["## TfidfVectorizer"]},{"cell_type":"code","metadata":{"id":"pbMPkJsrBo_1"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# list of text documents\n","text = [\"The quick brown fox jumped over the lazy dog.\", \"The dog.\", \"The fox\"]\n","\n","# create the transform\n","vectorizer = TfidfVectorizer()\n","\n","# tokenize and build vocab\n","vectorizer.fit(text)\n","\n","# summarize\n","print(vectorizer.vocabulary_)\n","print(vectorizer.idf_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYHUT9z-Bwk-"},"source":["# encode document\n","vector = vectorizer.transform([text[2]])   #<-- try to encode doc 0, 1, 2 \n","# summarize encoded vector\n","print(vector.shape)\n","print(vector.toarray())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ioPsvLdBHcnS"},"source":["## HashingVectorizer"]},{"cell_type":"code","metadata":{"id":"OEDsIYYrB2hJ"},"source":["from sklearn.feature_extraction.text import HashingVectorizer\n","\n","# list of text documents\n","text = [\"The quick brown fox jumped over the lazy dog.\"]\n","\n","# create the transform\n","vectorizer = HashingVectorizer(n_features=20)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RqUtyHYjHkI5"},"source":["# encode document\n","vector = vectorizer.transform(text)\n","\n","# summarize encoded vector\n","print(vector.shape)\n","print(vector.toarray())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vgYXf6NzxOcp"},"source":["# Handle text with Keras"]},{"cell_type":"markdown","metadata":{"id":"zUq2zmFLNIxF"},"source":["Use of `text_to_word_sequence()`.\n","\n"]},{"cell_type":"code","metadata":{"id":"2kNue42bxQJL"},"source":["from keras.preprocessing.text import text_to_word_sequence \n","\n","# define the document\n","text = 'The quick brown fox jumped over the lazy dog.'\n","\n","# tokenize the document\n","result = text_to_word_sequence(text)\n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5hrSIqU0xY2m"},"source":["from keras.preprocessing.text import text_to_word_sequence \n","\n","# define the document\n","text = 'The quick brown fox jumped over the lazy dog.'\n","\n","# estimate the size of the vocabulary\n","words = set(text_to_word_sequence(text))\n","vocab_size = len(words)\n","print(vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NxxoCRdeNQCI"},"source":["Use of `one_hot()`."]},{"cell_type":"code","metadata":{"id":"xuyX5Ow-MyHh"},"source":["from keras.preprocessing.text import text_to_word_sequence \n","from keras.preprocessing.text import one_hot\n","\n","# define the document\n","text = 'The quick brown fox jumped over the lazy dog.'\n","\n","# estimate the size of the vocabulary\n","words = set(text_to_word_sequence(text))\n","vocab_size = len(words)\n","print(vocab_size)\n","\n","# integer encode the document\n","result = one_hot(text, round(vocab_size*55.3))\n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eV2IMe6zQ4Y6"},"source":["Hash encoding with `hashing_trick()`."]},{"cell_type":"code","metadata":{"id":"gaGRwoJMNh1r"},"source":["from keras.preprocessing.text import text_to_word_sequence \n","from keras.preprocessing.text import hashing_trick\n","\n","# define the document\n","text = 'The quick brown fox jumped over the lazy dog.'\n","\n","# estimate the size of the vocabulary\n","words = set(text_to_word_sequence(text))\n","vocab_size = len(words)\n","print(vocab_size)\n","\n","# integer encode the document\n","result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5') \n","print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BKhRHrLaSgdh"},"source":["## Tokenizer API"]},{"cell_type":"code","metadata":{"id":"RCHzYnq1RIxJ"},"source":["from keras.preprocessing.text import Tokenizer \n","\n","# define 5 documents\n","docs = ['Well done!',\n","        'Good work', \n","        'Great effort', \n","        'nice work', \n","        'Excellent!']\n","\n","# create the tokenizer\n","t = Tokenizer()\n","\n","# fit the tokenizer on the documents\n","t.fit_on_texts(docs)\n","\n","# summarize what was learned\n","print(\"word count ->\", t.word_counts)\n","print(\"document count ->\", t.document_count)\n","print(\"word index ->\", t.word_index)\n","print(\"word docs ->\", t.word_docs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pvMjlPr4TDkn"},"source":["# integer encode documents\n","encoded_docs = t.texts_to_matrix(docs, mode='count')\n","print(encoded_docs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_7rrNVbauBDs"},"source":["# integer encode documents\n","encoded_docs = t.texts_to_matrix(docs, mode='freq')\n","print(encoded_docs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"So4dHdaIuOk5"},"source":["# integer encode documents\n","encoded_docs = t.texts_to_matrix(docs, mode='tfidf')\n","print(encoded_docs)"],"execution_count":null,"outputs":[]}]}